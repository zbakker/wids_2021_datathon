{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (130157, 179)\n",
      "Train label: (130157, 1)\n",
      "Test: (10234, 179)\n"
     ]
    }
   ],
   "source": [
    "#Input data\n",
    "train = pd.read_csv('TrainingWiDS2021.csv')\n",
    "test = pd.read_csv('UnlabeledWiDS2021.csv')\n",
    "test = test.sort_values(by='encounter_id') \n",
    "\n",
    "\n",
    "# split out train labels\n",
    "train_labels = train[['diabetes_mellitus']]\n",
    "train = train.drop(['Unnamed: 0', 'diabetes_mellitus'], axis=1)\n",
    "test = test.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Train label:\", train_labels.shape)\n",
    "print(\"Test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int64': Index(['encounter_id', 'hospital_id', 'elective_surgery', 'icu_id',\n",
       "        'readmission_status', 'apache_post_operative', 'arf_apache',\n",
       "        'intubated_apache', 'ventilated_apache', 'aids', 'cirrhosis',\n",
       "        'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma',\n",
       "        'solid_tumor_with_metastasis'],\n",
       "       dtype='object'),\n",
       " 'float64': Index(['age', 'bmi', 'height', 'pre_icu_los_days', 'weight', 'albumin_apache',\n",
       "        'apache_2_diagnosis', 'apache_3j_diagnosis', 'bilirubin_apache',\n",
       "        'bun_apache',\n",
       "        ...\n",
       "        'd1_pao2fio2ratio_max', 'd1_pao2fio2ratio_min', 'h1_arterial_pco2_max',\n",
       "        'h1_arterial_pco2_min', 'h1_arterial_ph_max', 'h1_arterial_ph_min',\n",
       "        'h1_arterial_po2_max', 'h1_arterial_po2_min', 'h1_pao2fio2ratio_max',\n",
       "        'h1_pao2fio2ratio_min'],\n",
       "       dtype='object', length=157),\n",
       " 'object': Index(['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',\n",
       "        'icu_stay_type', 'icu_type'],\n",
       "       dtype='object')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_by_dtype = train.columns.to_series().groupby(train.dtypes).groups\n",
    "cols_by_dtype = {k.name: v for k, v in cols_by_dtype.items()}\n",
    "cols_by_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_val_percent(df, thresh = 70.0):\n",
    "    \"\"\"\n",
    "    Remove all features that have a missing value percentage greater than 70%\n",
    "    \"\"\"\n",
    "    mvp = ((df.isnull().sum()) / (len(df))) * 100\n",
    "    return mvp[mvp > thresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (130157, 124)\n",
      "Test: (10234, 124)\n"
     ]
    }
   ],
   "source": [
    "# create retained features list\n",
    "retained_features = set(train.columns) - set(miss_val_percent(train).index)\n",
    "\n",
    "# subset train and test sets\n",
    "train = train[retained_features]\n",
    "test = test[retained_features]\n",
    "\n",
    "# check\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int64': Index(['cirrhosis', 'aids', 'solid_tumor_with_metastasis', 'intubated_apache',\n",
       "        'leukemia', 'arf_apache', 'lymphoma', 'encounter_id',\n",
       "        'readmission_status', 'hepatic_failure', 'immunosuppression',\n",
       "        'apache_post_operative', 'icu_id', 'hospital_id', 'ventilated_apache',\n",
       "        'elective_surgery'],\n",
       "       dtype='object'),\n",
       " 'float64': Index(['h1_temp_max', 'd1_mbp_noninvasive_min', 'heart_rate_apache',\n",
       "        'd1_heartrate_min', 'h1_diasbp_max', 'd1_diasbp_min', 'd1_spo2_min',\n",
       "        'h1_mbp_noninvasive_min', 'd1_bun_max', 'd1_platelets_max',\n",
       "        ...\n",
       "        'gcs_unable_apache', 'd1_temp_max', 'd1_sodium_min',\n",
       "        'd1_mbp_noninvasive_max', 'd1_diasbp_noninvasive_max', 'd1_calcium_min',\n",
       "        'weight', 'h1_sysbp_max', 'd1_resprate_max', 'urineoutput_apache'],\n",
       "       dtype='object', length=102),\n",
       " 'object': Index(['ethnicity', 'hospital_admit_source', 'icu_stay_type', 'gender',\n",
       "        'icu_admit_source', 'icu_type'],\n",
       "       dtype='object')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_by_dtype = train.columns.to_series().groupby(train.dtypes).groups\n",
    "cols_by_dtype = {k.name: v for k, v in cols_by_dtype.items()}\n",
    "cols_by_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_miss_percent(df):\n",
    "    \"\"\"\n",
    "    Adds a feature into the dataframe indicating the missing\n",
    "    value percentage of the corresponding row\n",
    "    \"\"\"\n",
    "    ncols = df.shape[1]\n",
    "    df['miss_percent'] = (df.isnull().sum(axis=1) / ncols) * 100\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (119132, 125)\n",
      "Test: (10234, 125)\n"
     ]
    }
   ],
   "source": [
    "# calculate missing percent per row for train and test\n",
    "get_row_miss_percent(train)\n",
    "get_row_miss_percent(test)\n",
    "\n",
    "# subset train\n",
    "train = train.loc[train['miss_percent'] < 30]\n",
    "\n",
    "# check\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the categorical imputer object\n",
    "cat_imputer = SimpleImputer(missing_values = np.nan, strategy = \"constant\", fill_value = \"missing\")\n",
    "\n",
    "# subset data to have only categorical features\n",
    "train_cat = train[cols_by_dtype['object']]\n",
    "test_cat = test[cols_by_dtype['object']]\n",
    "\n",
    "# impute\n",
    "train_cat.iloc[:, :] = cat_imputer.fit_transform(train_cat)\n",
    "test_cat.iloc[:, :] = cat_imputer.fit_transform(test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cont_imputer = SimpleImputer(missing_values = np.nan, strategy = \"median\")\n",
    "\n",
    "# subset data to have only continuous features\n",
    "train_cont_int = train[cols_by_dtype['int64']]\n",
    "train_cont_float = train[cols_by_dtype['float64']]\n",
    "test_cont_int = test[cols_by_dtype['int64']]\n",
    "test_cont_float = test[cols_by_dtype['float64']]\n",
    "\n",
    "# impute\n",
    "train_cont_int.iloc[:, :] = cont_imputer.fit_transform(train_cont_int)\n",
    "train_cont_float.iloc[:, :] = cont_imputer.fit_transform(train_cont_float)\n",
    "test_cont_int.iloc[:, :] = cont_imputer.fit_transform(test_cont_int)\n",
    "test_cont_float.iloc[:, :] = cont_imputer.fit_transform(test_cont_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (119132, 124)\n",
      "Test: (10234, 124)\n"
     ]
    }
   ],
   "source": [
    "new_train = pd.concat([train_cat, train_cont_int, train_cont_float], axis=1)\n",
    "new_test = pd.concat([test_cat, test_cont_int, test_cont_float], axis=1)\n",
    "\n",
    "# check\n",
    "print(\"Train:\", new_train.shape)\n",
    "print(\"Test:\", new_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: (119132, 1)\n"
     ]
    }
   ],
   "source": [
    "new_train_labels = train_labels.iloc[train.index]\n",
    "\n",
    "# check\n",
    "print(\"Train labels:\", new_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (119132, 156)\n",
      "Test: (10234, 156)\n"
     ]
    }
   ],
   "source": [
    "# one hot encode data\n",
    "new_train = pd.get_dummies(new_train)\n",
    "new_test = pd.get_dummies(new_test)\n",
    "\n",
    "# remove cols from train that are not present in test\n",
    "new_train = new_train[new_test.columns]\n",
    "\n",
    "# check\n",
    "print(\"Train:\", new_train.shape)\n",
    "print(\"Test:\", new_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (119132, 155)\n",
      "Test: (10234, 155)\n"
     ]
    }
   ],
   "source": [
    "new_train = new_train.drop([\"encounter_id\"],axis=1)\n",
    "new_test = new_test.drop([\"encounter_id\"],axis=1)\n",
    "\n",
    "# create standard scaler object with training data\n",
    "scaler = StandardScaler().fit(new_train)\n",
    "\n",
    "# apply scaler to train and test data\n",
    "new_train = pd.DataFrame(scaler.transform(new_train))\n",
    "new_test = pd.DataFrame(scaler.transform(new_test))\n",
    "\n",
    "# check\n",
    "print(\"Train:\", new_train.shape)\n",
    "print(\"Test:\", new_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, eval_metric='error',\n",
       "              gamma=0.5, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=6, min_child_weight=0.75, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=True,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After tuning model on subset of training data, train on the entire labeled dataset\n",
    "X_train= new_train\n",
    "y_train = new_train_labels\n",
    "  \n",
    "#Make XGBoost Classifier\n",
    "xgb_clf = XGBClassifier(eval_metric = \"error\", learning_rate = 0.1, colsample_bytree = 0.8, gamma =0.5, min_child_weight=0.75)\n",
    "\n",
    "# fit classifier\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict for unlabeled data\n",
    "y_score = xgb_clf.predict_proba(new_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['encounter_id'] = test['encounter_id']\n",
    "submission['diabetes_mellitus'] = y_score\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
